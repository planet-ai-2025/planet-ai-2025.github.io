{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "IFz7paz5YPXz",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pTiSjRjEsAm"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U transformers datasets accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NpYepezXZzO"
      },
      "outputs": [],
      "source": [
        "!pip install peft bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpSvwsF4pnJb"
      },
      "outputs": [],
      "source": [
        "!pip install ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnXzrzEpN2e3"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYnClr9oN4Y9"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "ds = load_dataset('henrik-dra/energy-meter')\n",
        "print(ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPMA3yVbOQco"
      },
      "outputs": [],
      "source": [
        "ds_train=ds['train']\n",
        "ds_test=ds['test']\n",
        "\n",
        "ds_test[11]\n",
        "ds_test[11]['image']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3RRBwlkWddl"
      },
      "outputs": [],
      "source": [
        "ds_test[11]['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cv24MbS5OWqc"
      },
      "outputs": [],
      "source": [
        "from transformers import PaliGemmaProcessor\n",
        "model_id = \"google/paligemma-3b-pt-224\"\n",
        "processor = PaliGemmaProcessor.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-cWipJ7l87Fk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import builtins\n",
        "builtins.np = np\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "def collate_fn(examples):\n",
        "  texts = [\"extract meter value from this image\" for example in examples]\n",
        "  labels= [example['label'] for example in examples]\n",
        "  images = [example[\"image\"].convert(\"RGB\") for example in examples]\n",
        "  tokens = processor(text=texts, images=images, suffix=labels,\n",
        "                    return_tensors=\"pt\", padding=\"longest\")\n",
        "\n",
        "  tokens = tokens.to(torch.bfloat16).to(device)\n",
        "  return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbHMzQMpOir1"
      },
      "outputs": [],
      "source": [
        "from transformers import PaliGemmaForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "model = PaliGemmaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.bfloat16).to(device)\n",
        "\n",
        "for param in model.vision_tower.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for param in model.multi_modal_projector.parameters():\n",
        "    param.requires_grad = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xE3VjeCxOmkV"
      },
      "outputs": [],
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "from peft import get_peft_model, LoraConfig\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_type=torch.bfloat16\n",
        ")\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "model = PaliGemmaForConditionalGeneration.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "bnQGqVZCOqDJ"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "args=TrainingArguments(\n",
        "            num_train_epochs=2,\n",
        "            remove_unused_columns=False,\n",
        "            per_device_train_batch_size=2,\n",
        "            gradient_accumulation_steps=2,\n",
        "            warmup_steps=2,\n",
        "            learning_rate=2e-5,\n",
        "            weight_decay=1e-6,\n",
        "            adam_beta2=0.999,\n",
        "            logging_steps=100,\n",
        "            optim=\"adamw_torch\",\n",
        "            save_strategy=\"steps\",\n",
        "            save_steps=138,\n",
        "            push_to_hub=False,\n",
        "            save_total_limit=1,\n",
        "            output_dir=\"paligemma_meter\",\n",
        "            bf16=True,\n",
        "            report_to=[\"tensorboard\"],\n",
        "            dataloader_pin_memory=False\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXzHQOAOOrhB"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "        model=model,\n",
        "        train_dataset=ds_train ,\n",
        "        eval_dataset=ds_test,\n",
        "        data_collator=collate_fn,\n",
        "        args=args\n",
        "        )\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install --upgrade pip\n",
        "!pip -q install \"transformers>=4.42.0\" accelerate bitsandbytes gradio pillow safetensors\n",
        "!pip install gradio\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw\n",
        "import torch\n",
        "from transformers import AutoProcessor, PaliGemmaForConditionalGeneration, BitsAndBytesConfig\n",
        "import gradio as gr\n",
        "\n",
        "\n",
        "# If you have a local fine-tuned checkpoint, set it here, e.g.:\n",
        "MODEL_ID = \"/content/drive/MyDrive/paligemma_meter/checkpoint-70\"\n",
        "#MODEL_ID = None  # Leave None to use the base model below.\n",
        "BASE_MODEL = \"google/paligemma-3b-pt-224\"\n",
        "BASE_PROCESSOR_ID = BASE_MODEL\n",
        "\n",
        "# Use 4-bit quantization only when CUDA is available\n",
        "use_4bit = torch.cuda.is_available()\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ") if use_4bit else None\n",
        "\n",
        "chosen_model_id = MODEL_ID if MODEL_ID else BASE_MODEL\n",
        "print(f\"Loading model from: {chosen_model_id}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}  |  4-bit: {bool(bnb_config)}\")\n",
        "\n",
        "# ---------------------------\n",
        "# Load model & processor\n",
        "# ---------------------------\n",
        "dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
        "    chosen_model_id,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config,\n",
        "    torch_dtype=dtype\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(BASE_PROCESSOR_ID)\n",
        "\n",
        "eos_token_id = processor.tokenizer.eos_token_id\n",
        "pad_token_id = processor.tokenizer.pad_token_id or eos_token_id\n",
        "\n",
        "print(\"Model and processor ready.\")\n",
        "\n",
        "# ---------------------------\n",
        "# Inference helpers\n",
        "# ---------------------------\n",
        "def generate_response(question, image):\n",
        "    if not question:\n",
        "        return \"Please enter a question.\"\n",
        "    if image is None:\n",
        "        return \"Please upload an image.\"\n",
        "\n",
        "    # Accept numpy array or PIL.Image\n",
        "    if isinstance(image, np.ndarray):\n",
        "        raw_image = Image.fromarray(image).convert(\"RGB\")\n",
        "    elif isinstance(image, Image.Image):\n",
        "        raw_image = image.convert(\"RGB\")\n",
        "    else:\n",
        "        return \"Unsupported image type. Please upload a standard image.\"\n",
        "\n",
        "    # A short instruction tends to help PaliGemma be concise\n",
        "    prompt = f\"Answer briefly: {question}\"\n",
        "\n",
        "    inputs = processor(text=prompt, images=raw_image, return_tensors=\"pt\")\n",
        "    # Move to the same device(s) as the model\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        output_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=64,\n",
        "            do_sample=False,\n",
        "            eos_token_id=eos_token_id,\n",
        "            pad_token_id=pad_token_id,\n",
        "        )\n",
        "\n",
        "    # Decode only the new tokens after the prompt\n",
        "    input_len = inputs[\"input_ids\"].shape[-1]\n",
        "    gen_ids = output_ids[0][input_len:]\n",
        "    response = processor.batch_decode(gen_ids.unsqueeze(0), skip_special_tokens=True)[0].strip()\n",
        "\n",
        "    if not response:\n",
        "        # Fallback if nothing new was generated or the model echoed prompt\n",
        "        response = processor.batch_decode(output_ids, skip_special_tokens=True)[0].replace(prompt, \"\").strip()\n",
        "\n",
        "    return response or \"(no answer generated)\"\n",
        "\n",
        "def make_local_examples(root=\"/tmp/vlm_examples\"):\n",
        "    os.makedirs(root, exist_ok=True)\n",
        "\n",
        "    # Simple, synthetic images so we don't rely on the network\n",
        "    def make_traffic_light(path):\n",
        "        img = Image.new(\"RGB\", (512, 512), \"gray\")\n",
        "        d = ImageDraw.Draw(img)\n",
        "        d.rectangle([200, 80, 312, 440], fill=\"black\")\n",
        "        d.ellipse([215, 95, 297, 177], fill=\"red\")\n",
        "        d.ellipse([215, 205, 297, 287], fill=\"yellow\")\n",
        "        d.ellipse([215, 315, 297, 397], fill=\"green\")\n",
        "        img.save(path)\n",
        "\n",
        "    def make_people_count(path, n=3):\n",
        "        img = Image.new(\"RGB\", (512, 512), \"white\")\n",
        "        d = ImageDraw.Draw(img)\n",
        "        xs = [100, 256, 412][:n]\n",
        "        for x in xs:\n",
        "            d.ellipse([x-30, 120-30, x+30, 120+30], fill=\"black\")    # head\n",
        "            d.rectangle([x-20, 150, x+20, 300], fill=\"black\")        # torso\n",
        "            d.line([x-20, 200, x-50, 260], fill=\"black\", width=12)   # left arm\n",
        "            d.line([x+20, 200, x+50, 260], fill=\"black\", width=12)   # right arm\n",
        "            d.line([x-10, 300, x-40, 380], fill=\"black\", width=12)   # left leg\n",
        "            d.line([x+10, 300, x+40, 380], fill=\"black\", width=12)   # right leg\n",
        "        img.save(path)\n",
        "\n",
        "    def make_animal_action(path):\n",
        "        img = Image.new(\"RGB\", (512, 512), \"skyblue\")\n",
        "        d = ImageDraw.Draw(img)\n",
        "        d.rectangle([0, 380, 512, 512], fill=\"green\")                # ground\n",
        "        d.rectangle([260, 350, 320, 380], fill=\"brown\")              # obstacle\n",
        "        d.ellipse([120, 260, 240, 340], fill=\"saddlebrown\")          # body\n",
        "        d.ellipse([220, 250, 260, 290], fill=\"saddlebrown\")          # head\n",
        "        d.polygon([(240,300),(270,290),(260,320)], fill=\"saddlebrown\")  # tail\n",
        "        d.arc([90, 230, 300, 400], 320, 20, width=4, fill=\"white\")   # motion arc\n",
        "        img.save(path)\n",
        "\n",
        "    p1 = os.path.join(root, \"traffic_light.jpg\")\n",
        "    p2 = os.path.join(root, \"people.jpg\")\n",
        "    p3 = os.path.join(root, \"animal_jump.jpg\")\n",
        "\n",
        "    if not os.path.exists(p1): make_traffic_light(p1)\n",
        "    if not os.path.exists(p2): make_people_count(p2, n=3)\n",
        "    if not os.path.exists(p3): make_animal_action(p3)\n",
        "\n",
        "    return p1, p2, p3\n",
        "\n",
        "# ---------------------------\n",
        "# Gradio UI\n",
        "# ---------------------------\n",
        "p1, p2, p3 = make_local_examples()\n",
        "\n",
        "def chat_interface(question, image):\n",
        "    return generate_response(question, image)\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Fine-tuned PaliGemma (VQA Demo)\")\n",
        "    gr.Markdown(\"Upload an image and ask a concise question about it.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            image = gr.Image(label=\"Upload Image\", type=\"numpy\")\n",
        "        with gr.Column(scale=1):\n",
        "            question = gr.Textbox(label=\"Question\", placeholder=\"e.g., What color is the traffic light?\", lines=1)\n",
        "            predict_button = gr.Button(\"Predict\", variant=\"primary\")\n",
        "            response = gr.Textbox(label=\"Response\")\n",
        "\n",
        "    gr.Examples(\n",
        "        examples=[\n",
        "            [\"What color is the traffic light?\", p1],\n",
        "            [\"How many people are in the photo?\", p2],\n",
        "            [\"What is the animal doing?\", p3],\n",
        "        ],\n",
        "        inputs=[question, image],\n",
        "        cache_examples=False  # avoid prefetching that might hit the network\n",
        "    )\n",
        "\n",
        "    predict_button.click(fn=chat_interface, inputs=[question, image], outputs=response)\n",
        "\n",
        "demo.launch(share=True, debug=True)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "tVWcC1NtbQxU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}