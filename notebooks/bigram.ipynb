{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization: split() uses whitespace only; punctuation (if present) would stay attached. For real tasks you might want a better tokenizer.\n",
        "\n",
        "Defaultdict: saves you from writing if bigram in dict: ... else: ...â€”missing counts start at 0.\n",
        "\n",
        "Ties: When multiple next-words have the same frequency (e.g., after 'love'), the earliest one encountered during counting is returned.\n",
        "\n",
        "Out-of-vocabulary: If you ask for a word that never appears as the first element of any bigram, the function returns None."
      ],
      "metadata": {
        "id": "0YrqexLJng7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports defaultdict which supplies a default value (like 0) for missing keys.\n",
        "\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "AOGW0mG0VKLg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Dataset"
      ],
      "metadata": {
        "id": "7zzzPPV6VLzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A tiny corpus: a list of sentences (strings). We'll learn bigrams from this.\n",
        "\n",
        "dataset = [\n",
        "    \"Though the righteous fall seven times, they rise again, but the wicked stumble when calamity strikes.\",\n",
        "    \"Do not gloat when your enemy falls; when they stumble, do not let your heart rejoice.\",\n",
        "    \"Do not envy the wicked; do not desire their company.\",\n",
        "    \"Do not fret because of evildoers or be envious of the wicked, for the evildoer has no future hope, and the lamp of the wicked will be snuffed out.\",\n",
        "    \"By wisdom a house is built, and through understanding it is established; through knowledge its rooms are filled with rare and beautiful treasures.\"\n",
        "]"
      ],
      "metadata": {
        "id": "6wTZAYTeVQ0u"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70FEXvPPakEQ",
        "outputId": "08863822-8d26-4442-ba9f-b1c5b870b265"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Though the righteous fall seven times, they rise again, but the wicked stumble when calamity strikes.',\n",
              " 'Do not gloat when your enemy falls; when they stumble, do not let your heart rejoice.',\n",
              " 'Do not envy the wicked; do not desire their company.',\n",
              " 'Do not fret because of evildoers or be envious of the wicked, for the evildoer has no future hope, and the lamp of the wicked will be snuffed out.',\n",
              " 'By wisdom a house is built, and through understanding it is established; through knowledge its rooms are filled with rare and beautiful treasures.']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Tokenize sentences"
      ],
      "metadata": {
        "id": "0TQJicNhVVy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List comprehension:\n",
        "# - sentence.lower(): make everything lowercase so 'Do' and 'do' are treated the same.\n",
        "# - .split(): split on whitespace into tokens (words).\n",
        "# Result: list of lists, e.g. [[\"do\",\"not\",\"envy\",\"the\",\"wicked\"], ...]\n",
        "\n",
        "tokenized_data = [sentence.lower().split() for sentence in dataset]"
      ],
      "metadata": {
        "id": "ay18NUBFVadj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJkW0s08bKp-",
        "outputId": "a7d8c4c3-96ad-4288-9439-70aefaa96c81"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['though',\n",
              "  'the',\n",
              "  'righteous',\n",
              "  'fall',\n",
              "  'seven',\n",
              "  'times,',\n",
              "  'they',\n",
              "  'rise',\n",
              "  'again,',\n",
              "  'but',\n",
              "  'the',\n",
              "  'wicked',\n",
              "  'stumble',\n",
              "  'when',\n",
              "  'calamity',\n",
              "  'strikes.'],\n",
              " ['do',\n",
              "  'not',\n",
              "  'gloat',\n",
              "  'when',\n",
              "  'your',\n",
              "  'enemy',\n",
              "  'falls;',\n",
              "  'when',\n",
              "  'they',\n",
              "  'stumble,',\n",
              "  'do',\n",
              "  'not',\n",
              "  'let',\n",
              "  'your',\n",
              "  'heart',\n",
              "  'rejoice.'],\n",
              " ['do',\n",
              "  'not',\n",
              "  'envy',\n",
              "  'the',\n",
              "  'wicked;',\n",
              "  'do',\n",
              "  'not',\n",
              "  'desire',\n",
              "  'their',\n",
              "  'company.'],\n",
              " ['do',\n",
              "  'not',\n",
              "  'fret',\n",
              "  'because',\n",
              "  'of',\n",
              "  'evildoers',\n",
              "  'or',\n",
              "  'be',\n",
              "  'envious',\n",
              "  'of',\n",
              "  'the',\n",
              "  'wicked,',\n",
              "  'for',\n",
              "  'the',\n",
              "  'evildoer',\n",
              "  'has',\n",
              "  'no',\n",
              "  'future',\n",
              "  'hope,',\n",
              "  'and',\n",
              "  'the',\n",
              "  'lamp',\n",
              "  'of',\n",
              "  'the',\n",
              "  'wicked',\n",
              "  'will',\n",
              "  'be',\n",
              "  'snuffed',\n",
              "  'out.'],\n",
              " ['by',\n",
              "  'wisdom',\n",
              "  'a',\n",
              "  'house',\n",
              "  'is',\n",
              "  'built,',\n",
              "  'and',\n",
              "  'through',\n",
              "  'understanding',\n",
              "  'it',\n",
              "  'is',\n",
              "  'established;',\n",
              "  'through',\n",
              "  'knowledge',\n",
              "  'its',\n",
              "  'rooms',\n",
              "  'are',\n",
              "  'filled',\n",
              "  'with',\n",
              "  'rare',\n",
              "  'and',\n",
              "  'beautiful',\n",
              "  'treasures.']]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Build bigram counts"
      ],
      "metadata": {
        "id": "Rea28RImVeWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_counts = defaultdict(int)\n",
        "# Create a dictionary mapping (word1, word2) -> count.\n",
        "# Using defaultdict(int) means unseen keys start at 0 automatically.\n",
        "\n",
        "for sentence in tokenized_data:\n",
        "# Loop over each tokenized sentence (a list of words).\n",
        "\n",
        "    for i in range(len(sentence) - 1):\n",
        "        # Iterate over indices where a bigram exists.\n",
        "        # If a sentence has N words, it has N-1 bigrams (pairs of consecutive words).\n",
        "\n",
        "        bigram = (sentence[i], sentence[i+1])   # Build the bigram tuple: current word and the next word.\n",
        "        bigram_counts[bigram] += 1     # Increment the count for this bigram.\n",
        "\n",
        "print(\"Bigram frequencies:\\n\")\n",
        "for bigram, count in bigram_counts.items():    # Iterate through all learned bigrams and their counts.\n",
        "    print(bigram, \":\", count)     # Print each bigram as a tuple and its frequency."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TisPOSJEVi3S",
        "outputId": "6c9355a7-25fb-4c4d-c74d-d544cf1e4191"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigram frequencies:\n",
            "\n",
            "('though', 'the') : 1\n",
            "('the', 'righteous') : 1\n",
            "('righteous', 'fall') : 1\n",
            "('fall', 'seven') : 1\n",
            "('seven', 'times,') : 1\n",
            "('times,', 'they') : 1\n",
            "('they', 'rise') : 1\n",
            "('rise', 'again,') : 1\n",
            "('again,', 'but') : 1\n",
            "('but', 'the') : 1\n",
            "('the', 'wicked') : 2\n",
            "('wicked', 'stumble') : 1\n",
            "('stumble', 'when') : 1\n",
            "('when', 'calamity') : 1\n",
            "('calamity', 'strikes.') : 1\n",
            "('do', 'not') : 5\n",
            "('not', 'gloat') : 1\n",
            "('gloat', 'when') : 1\n",
            "('when', 'your') : 1\n",
            "('your', 'enemy') : 1\n",
            "('enemy', 'falls;') : 1\n",
            "('falls;', 'when') : 1\n",
            "('when', 'they') : 1\n",
            "('they', 'stumble,') : 1\n",
            "('stumble,', 'do') : 1\n",
            "('not', 'let') : 1\n",
            "('let', 'your') : 1\n",
            "('your', 'heart') : 1\n",
            "('heart', 'rejoice.') : 1\n",
            "('not', 'envy') : 1\n",
            "('envy', 'the') : 1\n",
            "('the', 'wicked;') : 1\n",
            "('wicked;', 'do') : 1\n",
            "('not', 'desire') : 1\n",
            "('desire', 'their') : 1\n",
            "('their', 'company.') : 1\n",
            "('not', 'fret') : 1\n",
            "('fret', 'because') : 1\n",
            "('because', 'of') : 1\n",
            "('of', 'evildoers') : 1\n",
            "('evildoers', 'or') : 1\n",
            "('or', 'be') : 1\n",
            "('be', 'envious') : 1\n",
            "('envious', 'of') : 1\n",
            "('of', 'the') : 2\n",
            "('the', 'wicked,') : 1\n",
            "('wicked,', 'for') : 1\n",
            "('for', 'the') : 1\n",
            "('the', 'evildoer') : 1\n",
            "('evildoer', 'has') : 1\n",
            "('has', 'no') : 1\n",
            "('no', 'future') : 1\n",
            "('future', 'hope,') : 1\n",
            "('hope,', 'and') : 1\n",
            "('and', 'the') : 1\n",
            "('the', 'lamp') : 1\n",
            "('lamp', 'of') : 1\n",
            "('wicked', 'will') : 1\n",
            "('will', 'be') : 1\n",
            "('be', 'snuffed') : 1\n",
            "('snuffed', 'out.') : 1\n",
            "('by', 'wisdom') : 1\n",
            "('wisdom', 'a') : 1\n",
            "('a', 'house') : 1\n",
            "('house', 'is') : 1\n",
            "('is', 'built,') : 1\n",
            "('built,', 'and') : 1\n",
            "('and', 'through') : 1\n",
            "('through', 'understanding') : 1\n",
            "('understanding', 'it') : 1\n",
            "('it', 'is') : 1\n",
            "('is', 'established;') : 1\n",
            "('established;', 'through') : 1\n",
            "('through', 'knowledge') : 1\n",
            "('knowledge', 'its') : 1\n",
            "('its', 'rooms') : 1\n",
            "('rooms', 'are') : 1\n",
            "('are', 'filled') : 1\n",
            "('filled', 'with') : 1\n",
            "('with', 'rare') : 1\n",
            "('rare', 'and') : 1\n",
            "('and', 'beautiful') : 1\n",
            "('beautiful', 'treasures.') : 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Predict next word given a word"
      ],
      "metadata": {
        "id": "ysnV5-CLWWKB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMXLxnY9U_3X",
        "outputId": "b04dfe8a-2606-4c9d-c19f-5dd0dfb1aef3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prediction examples:\n",
            "After 'do' -> not\n",
            "After 'calamity' -> strikes.\n",
            "After 'through' -> understanding\n"
          ]
        }
      ],
      "source": [
        "# Dictionary comprehension:\n",
        "# - Look at every bigram (k is a tuple like ('do','not')), v is its count.\n",
        "# - Keep only those whose first word (k[0]) matches the input 'word'.\n",
        "# - Map each candidate next word (k[1]) to its count.\n",
        "# Example: if word == 'd', candidates might be {'not': 5}.\n",
        "\n",
        "def predict_next_word(word):\n",
        "    candidates = {k[1]: v for k, v in bigram_counts.items() if k[0] == word}\n",
        "    if not candidates:\n",
        "        return None\n",
        "    return max(candidates, key=candidates.get)\n",
        "\n",
        "# Test prediction\n",
        "print(\"\\nPrediction examples:\")\n",
        "print(\"After 'do' ->\", predict_next_word(\"do\"))\n",
        "print(\"After 'calamity' ->\", predict_next_word(\"calamity\"))\n",
        "print(\"After 'through' ->\", predict_next_word(\"through\"))"
      ]
    }
  ]
}